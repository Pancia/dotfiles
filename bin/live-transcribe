#!/Users/anthony/.pyenv/versions/3.11.11/bin/python3

import sounddevice as sd
import numpy as np
import mlx_whisper
import webrtcvad
import queue
import threading
import argparse
import sys
from datetime import datetime
import os
from pathlib import Path
import subprocess

# Configuration
SAMPLE_RATE = 16000  # Whisper expects 16kHz
VAD_FRAME_DURATION = 30  # ms - WebRTC VAD supports 10, 20, or 30ms
VAD_AGGRESSIVENESS = 2  # 0-3, where 3 is most aggressive
SILENCE_DURATION = 1.0  # seconds of silence before chunking
MIN_CHUNK_DURATION = 2.0  # minimum chunk duration in seconds
MAX_CHUNK_DURATION = 10.0  # maximum chunk duration in seconds

class LiveTranscriber:
    def __init__(self, model_size="base", output_dir=None, mic_device=None, computer_device=None,
                 vad_aggressiveness=VAD_AGGRESSIVENESS, silence_duration=SILENCE_DURATION,
                 min_chunk_duration=MIN_CHUNK_DURATION, max_chunk_duration=MAX_CHUNK_DURATION):
        # Set model path for MLX (optimized for Apple Silicon)
        # Map model sizes to correct HuggingFace repo names
        model_map = {
            "tiny": "mlx-community/whisper-tiny",
            "base": "mlx-community/whisper-base.en-mlx",  # Using English-only for speed
            "small": "mlx-community/whisper-small-mlx",
            "medium": "mlx-community/whisper-medium-mlx",
            "large": "mlx-community/whisper-large-v3-mlx",
            "turbo": "mlx-community/whisper-turbo",
            "large-v3-turbo": "mlx-community/whisper-large-v3-turbo"
        }

        self.model_path = model_map.get(model_size, f"mlx-community/whisper-{model_size}")
        print(f"Using {self.model_path} with MLX (Apple Silicon optimized)...", file=sys.stderr)
        self.audio_queue = queue.Queue()
        self.running = True

        # Store VAD parameters
        self.vad_aggressiveness = vad_aggressiveness
        self.silence_duration = silence_duration
        self.min_chunk_duration = min_chunk_duration
        self.max_chunk_duration = max_chunk_duration

        # Setup output directory and file
        if output_dir is None:
            output_dir = Path.home() / "transcripts"
        else:
            output_dir = Path(output_dir)

        output_dir.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        self.output_file = output_dir / f"transcript_{timestamp}.txt"

        # Open file for writing
        self.file_handle = open(self.output_file, 'w')
        print(f"Writing transcript to: {self.output_file}", file=sys.stderr)

        # Device configuration
        self.mic_device = mic_device
        self.computer_device = computer_device

        # Query device info to get channel counts and sample rates
        devices = sd.query_devices()
        self.mic_channels = self._get_device_channels(devices[mic_device])
        self.computer_channels = self._get_device_channels(devices[computer_device])

        # Get native sample rates and use them (we'll resample later for Whisper)
        self.mic_sample_rate = int(devices[mic_device]['default_samplerate'])
        self.computer_sample_rate = int(devices[computer_device]['default_samplerate'])

        print(f"Mic device: {devices[mic_device]['name']}", file=sys.stderr)
        print(f"  Channels: {self.mic_channels}, Sample rate: {self.mic_sample_rate} Hz", file=sys.stderr)
        print(f"Computer device: {devices[computer_device]['name']}", file=sys.stderr)
        print(f"  Channels: {self.computer_channels}, Sample rate: {self.computer_sample_rate} Hz", file=sys.stderr)

        # Initialize VAD
        self.vad = webrtcvad.Vad(self.vad_aggressiveness)
        self.vad_frame_size = int(SAMPLE_RATE * VAD_FRAME_DURATION / 1000)  # samples per frame
        self.silence_frames_threshold = int(self.silence_duration * 1000 / VAD_FRAME_DURATION)

        # Audio buffers
        self.mic_buffer = []
        self.computer_buffer = []

        # VAD processing buffers (accumulate samples until we have enough for VAD)
        self.mic_vad_buffer = np.array([], dtype=np.float32)
        self.computer_vad_buffer = np.array([], dtype=np.float32)

        # Speech state tracking
        self.mic_speaking = False
        self.mic_silence_frames = 0
        self.mic_chunk_start_time = None

        self.computer_speaking = False
        self.computer_silence_frames = 0
        self.computer_chunk_start_time = None

    def _get_device_channels(self, device_info):
        """Get the appropriate number of channels for a device"""
        max_channels = device_info['max_input_channels']
        # Use stereo if available, otherwise mono
        return min(2, max_channels) if max_channels > 0 else 1

    def _to_mono(self, audio_data):
        """Convert stereo audio to mono by averaging channels"""
        if len(audio_data.shape) == 2 and audio_data.shape[1] == 2:
            return np.mean(audio_data, axis=1)
        return audio_data.flatten()

    def _resample(self, audio_data, orig_sr):
        """Resample audio to 16kHz for Whisper"""
        if orig_sr == SAMPLE_RATE:
            return audio_data

        # Simple resampling using numpy
        duration = len(audio_data) / orig_sr
        target_length = int(duration * SAMPLE_RATE)

        # Linear interpolation for resampling
        indices = np.linspace(0, len(audio_data) - 1, target_length)
        resampled = np.interp(indices, np.arange(len(audio_data)), audio_data)

        return resampled.astype(np.float32)

    def _is_speech(self, audio_frame):
        """Check if audio frame contains speech using VAD"""
        # Convert float32 to int16 for WebRTC VAD
        audio_int16 = (audio_frame * 32768).astype(np.int16)
        # WebRTC VAD expects bytes
        audio_bytes = audio_int16.tobytes()

        try:
            return self.vad.is_speech(audio_bytes, SAMPLE_RATE)
        except Exception:
            # If VAD fails, assume it's speech to be safe
            return True

    def audio_callback_mic(self, indata, frames, time, status):
        """Callback for microphone audio with VAD-based chunking"""
        if status:
            print(f"Mic status: {status}", file=sys.stderr)

        # Convert incoming audio to mono and resample to 16kHz for VAD
        mono_audio = self._to_mono(indata.copy())
        resampled_audio = self._resample(mono_audio, self.mic_sample_rate)

        # Add to VAD buffer
        self.mic_vad_buffer = np.concatenate([self.mic_vad_buffer, resampled_audio])

        # Debug: Check audio level
        audio_level = np.abs(resampled_audio).mean()
        max_level = np.abs(resampled_audio).max()

        # Process VAD frames when we have enough samples
        speech_frames = 0
        silence_frames = 0
        while len(self.mic_vad_buffer) >= self.vad_frame_size:
            # Extract one VAD frame
            frame = self.mic_vad_buffer[:self.vad_frame_size]
            self.mic_vad_buffer = self.mic_vad_buffer[self.vad_frame_size:]

            # Noise gate: if frame is too quiet, treat as silence
            frame_level = np.abs(frame).max()
            if frame_level < 0.01:
                is_speech = False
            else:
                is_speech = self._is_speech(frame)

            if is_speech:
                speech_frames += 1
                if not self.mic_speaking:
                    # Transition to speaking - start new chunk
                    self.mic_speaking = True
                    self.mic_chunk_start_time = datetime.now()
                    # print(f"[VAD] Mic: Started speaking (level avg={audio_level:.4f}, max={max_level:.4f})", file=sys.stderr)
                self.mic_silence_frames = 0
            else:
                silence_frames += 1
                if self.mic_speaking:
                    self.mic_silence_frames += 1

        # Debug output when there's significant audio
        # if max_level > 0.01:
        #     print(f"[VAD] Mic: speech_frames={speech_frames}, silence_frames={silence_frames}, level avg={audio_level:.4f}, max={max_level:.4f}, speaking={self.mic_speaking}, buffer={len(self.mic_vad_buffer)}", file=sys.stderr)

        # Add incoming audio to buffer AFTER VAD processing
        self.mic_buffer.append(resampled_audio)

        # Calculate current chunk duration
        if self.mic_chunk_start_time:
            chunk_duration = (datetime.now() - self.mic_chunk_start_time).total_seconds()
        else:
            chunk_duration = 0

        # Determine if we should send the chunk
        should_send = False

        if self.mic_speaking:
            # Send if we've detected enough silence after speech
            if self.mic_silence_frames >= self.silence_frames_threshold and chunk_duration >= self.min_chunk_duration:
                should_send = True
            # Force send if chunk is too long
            elif chunk_duration >= self.max_chunk_duration:
                should_send = True

        if should_send and len(self.mic_buffer) > 0:
            # Concatenate all buffered audio chunks
            complete_audio = np.concatenate(self.mic_buffer)
            duration = len(complete_audio) / SAMPLE_RATE
            # print(f"[DEBUG] Sending mic chunk: {duration:.2f}s, silence_frames={self.mic_silence_frames}, chunk_dur={chunk_duration:.2f}s", file=sys.stderr)
            self.audio_queue.put(("Microphone", complete_audio.copy()))
            # Reset state
            self.mic_buffer = []
            self.mic_speaking = False
            self.mic_silence_frames = 0
            self.mic_chunk_start_time = None
        # elif self.mic_speaking and chunk_duration > 1:
        #     # Debug: show what's preventing send
        #     print(f"[DEBUG] Mic speaking: silence_frames={self.mic_silence_frames}/{self.silence_frames_threshold}, dur={chunk_duration:.2f}s/{self.min_chunk_duration}s", file=sys.stderr)

    def audio_callback_computer(self, indata, frames, time, status):
        """Callback for computer audio with VAD-based chunking"""
        if status:
            print(f"Computer status: {status}", file=sys.stderr)

        # Convert incoming audio to mono and resample to 16kHz for VAD
        mono_audio = self._to_mono(indata.copy())
        resampled_audio = self._resample(mono_audio, self.computer_sample_rate)

        # Add to VAD buffer
        self.computer_vad_buffer = np.concatenate([self.computer_vad_buffer, resampled_audio])

        # Process VAD frames when we have enough samples
        while len(self.computer_vad_buffer) >= self.vad_frame_size:
            # Extract one VAD frame
            frame = self.computer_vad_buffer[:self.vad_frame_size]
            self.computer_vad_buffer = self.computer_vad_buffer[self.vad_frame_size:]

            # Noise gate: if frame is too quiet, treat as silence
            frame_level = np.abs(frame).max()
            if frame_level < 0.01:
                is_speech = False
            else:
                is_speech = self._is_speech(frame)

            if is_speech:
                if not self.computer_speaking:
                    # Transition to speaking - start new chunk
                    self.computer_speaking = True
                    self.computer_chunk_start_time = datetime.now()
                self.computer_silence_frames = 0
            else:
                if self.computer_speaking:
                    self.computer_silence_frames += 1

        # Add incoming audio to buffer AFTER VAD processing
        self.computer_buffer.append(resampled_audio)

        # Calculate current chunk duration
        if self.computer_chunk_start_time:
            chunk_duration = (datetime.now() - self.computer_chunk_start_time).total_seconds()
        else:
            chunk_duration = 0

        # Determine if we should send the chunk
        should_send = False

        if self.computer_speaking:
            # Send if we've detected enough silence after speech
            if self.computer_silence_frames >= self.silence_frames_threshold and chunk_duration >= self.min_chunk_duration:
                should_send = True
            # Force send if chunk is too long
            elif chunk_duration >= self.max_chunk_duration:
                should_send = True

        if should_send and len(self.computer_buffer) > 0:
            # Concatenate all buffered audio chunks
            complete_audio = np.concatenate(self.computer_buffer)
            duration = len(complete_audio) / SAMPLE_RATE
            # print(f"[DEBUG] Sending computer chunk: {duration:.2f}s, silence_frames={self.computer_silence_frames}, chunk_dur={chunk_duration:.2f}s", file=sys.stderr)
            self.audio_queue.put(("Computer", complete_audio.copy()))
            # Reset state
            self.computer_buffer = []
            self.computer_speaking = False
            self.computer_silence_frames = 0
            self.computer_chunk_start_time = None

    def transcription_worker(self):
        """Worker thread that processes audio chunks from the queue"""
        last_text_by_source = {}  # Track last text per source to deduplicate

        while self.running or not self.audio_queue.empty():
            try:
                source, audio_data = self.audio_queue.get(timeout=1)

                # Transcribe the audio chunk using MLX
                result = mlx_whisper.transcribe(
                    audio_data,
                    path_or_hf_repo=self.model_path
                )

                # Process segments and deduplicate consecutive identical ones
                if 'segments' in result:
                    for segment in result['segments']:
                        text = segment['text'].strip()
                        if text:  # Only output non-empty transcriptions
                            # Skip if this is identical to the last output from this source
                            if last_text_by_source.get(source) == text:
                                continue

                            last_text_by_source[source] = text
                            timestamp = datetime.now().strftime("%H:%M:%S")
                            output_line = f"[{timestamp}] {source}: {text}"

                            # Output to both stdout and file
                            print(output_line, flush=True)
                            self.file_handle.write(output_line + "\n")
                            self.file_handle.flush()

                self.audio_queue.task_done()

            except queue.Empty:
                continue
            except Exception as e:
                print(f"Error transcribing: {e}", file=sys.stderr)

    def _flush_buffers(self):
        """Flush any remaining audio in buffers to the queue"""
        # Flush mic buffer if it has any data
        # Note: buffer already contains resampled 16kHz mono audio
        if self.mic_buffer:
            audio_data = np.concatenate(self.mic_buffer)
            self.mic_buffer = []
            self.audio_queue.put(("Microphone", audio_data))

        # Flush computer buffer if it has any data
        # Note: buffer already contains resampled 16kHz mono audio
        if self.computer_buffer:
            audio_data = np.concatenate(self.computer_buffer)
            self.computer_buffer = []
            self.audio_queue.put(("Computer", audio_data))

    def start(self):
        """Start live transcription"""
        # Start transcription worker thread
        transcription_thread = threading.Thread(target=self.transcription_worker)
        transcription_thread.start()

        try:
            # Start both audio streams with appropriate channel counts and sample rates
            with sd.InputStream(device=self.mic_device,
                              channels=self.mic_channels,
                              samplerate=self.mic_sample_rate,
                              callback=self.audio_callback_mic):
                with sd.InputStream(device=self.computer_device,
                                  channels=self.computer_channels,
                                  samplerate=self.computer_sample_rate,
                                  callback=self.audio_callback_computer):
                    print("Press Ctrl+C to stop recording", file=sys.stderr)
                    print("\033[42m Recording started... \033[0m\n", file=sys.stderr)
                    # Keep running until interrupted
                    while self.running:
                        sd.sleep(1000)

        except KeyboardInterrupt:
            print("\n\nStopping recording...", file=sys.stderr)
            self.running = False

        # Flush any remaining buffered audio
        print("Flushing remaining audio buffers...", file=sys.stderr)
        self._flush_buffers()

        # Wait for transcription thread to finish processing queue
        print("Processing remaining transcriptions...", file=sys.stderr)
        transcription_thread.join()

        # Close file
        self.file_handle.close()
        print(f"\nTranscript saved to: {self.output_file}", file=sys.stderr)

def get_current_audio_output():
    """Get the current system audio output device name"""
    try:
        result = subprocess.run(
            ["SwitchAudioSource", "-c"],
            capture_output=True,
            text=True,
            check=True
        )
        return result.stdout.strip()
    except (subprocess.CalledProcessError, FileNotFoundError):
        return None

def set_audio_output(device_name):
    """Set the system audio output device"""
    try:
        subprocess.run(
            ["SwitchAudioSource", "-s", device_name],
            capture_output=True,
            check=True
        )
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False

def find_device_by_name(name):
    """Find device index by name (case-insensitive substring match)"""
    devices = sd.query_devices()
    name_lower = name.lower()

    for i, device in enumerate(devices):
        if name_lower in device['name'].lower():
            return i

    raise ValueError(f"No device found matching '{name}'")

def list_devices():
    """List all available audio devices"""
    print("\nAvailable INPUT devices:")
    print("-" * 80)
    devices = sd.query_devices()
    for i, device in enumerate(devices):
        if device['max_input_channels'] > 0:
            print(f"{i}: {device['name']}")
            print(f"   Max input channels: {device['max_input_channels']}")
            print(f"   Default sample rate: {device['default_samplerate']}")
            print()

def main():
    parser = argparse.ArgumentParser(description="Live transcribe microphone and computer audio")
    parser.add_argument("--output-dir", "-o",
                       help="Directory to save transcript files (default: ~/transcripts)")
    parser.add_argument("--model", "-m",
                       default="base",
                       choices=["tiny", "base", "small", "medium", "large"],
                       help="Whisper model size (default: base)")
    parser.add_argument("--mic-device",
                       help="Microphone device ID or name substring (use --list-devices to see options)")
    parser.add_argument("--computer-device",
                       help="Computer audio device ID or name substring, e.g., 'BlackHole' (use --list-devices to see options)")
    parser.add_argument("--list-devices", "-l",
                       action="store_true",
                       help="List available audio devices and exit")
    parser.add_argument("--vad-aggressiveness",
                       type=int,
                       default=VAD_AGGRESSIVENESS,
                       choices=[0, 1, 2, 3],
                       help="VAD aggressiveness level (0-3, higher = more aggressive filtering, default: 2)")
    parser.add_argument("--silence-duration",
                       type=float,
                       default=SILENCE_DURATION,
                       help="Seconds of silence before chunking (default: 1.0)")
    parser.add_argument("--min-chunk-duration",
                       type=float,
                       default=MIN_CHUNK_DURATION,
                       help="Minimum chunk duration in seconds (default: 2.0)")
    parser.add_argument("--max-chunk-duration",
                       type=float,
                       default=MAX_CHUNK_DURATION,
                       help="Maximum chunk duration in seconds (default: 10.0)")
    parser.add_argument("--auto-switch-output",
                       action="store_true",
                       help="Automatically switch to Multi-Output Device and restore on exit")
    parser.add_argument("--output-device-name",
                       default="Multi-Output Device",
                       help="Name of output device to switch to (default: Multi-Output Device)")

    args = parser.parse_args()

    if args.list_devices:
        list_devices()
        return

    # Check if devices are specified
    if args.mic_device is None or args.computer_device is None:
        print("Error: You must specify both --mic-device and --computer-device", file=sys.stderr)
        print("Use --list-devices to see available options\n", file=sys.stderr)
        list_devices()
        sys.exit(1)

    # Resolve device names to indices if needed
    try:
        # Try to parse as integer first
        mic_device = int(args.mic_device)
    except ValueError:
        # Otherwise treat as device name
        try:
            mic_device = find_device_by_name(args.mic_device)
            print(f"Found mic device: {sd.query_devices(mic_device)['name']} (index {mic_device})", file=sys.stderr)
        except ValueError as e:
            print(f"Error: {e}", file=sys.stderr)
            list_devices()
            sys.exit(1)

    try:
        # Try to parse as integer first
        computer_device = int(args.computer_device)
    except ValueError:
        # Otherwise treat as device name
        try:
            computer_device = find_device_by_name(args.computer_device)
            print(f"Found computer device: {sd.query_devices(computer_device)['name']} (index {computer_device})", file=sys.stderr)
        except ValueError as e:
            print(f"Error: {e}", file=sys.stderr)
            list_devices()
            sys.exit(1)

    # Handle audio output switching
    original_output = None
    if args.auto_switch_output:
        original_output = get_current_audio_output()
        if original_output:
            print(f"Current output: {original_output}", file=sys.stderr)
            print(f"Switching to: {args.output_device_name}", file=sys.stderr)
            if set_audio_output(args.output_device_name):
                print(f"✓ Switched to {args.output_device_name}", file=sys.stderr)
            else:
                print(f"Warning: Failed to switch to {args.output_device_name}", file=sys.stderr)
                original_output = None  # Don't try to restore if we couldn't switch
        else:
            print("Warning: Could not detect current output device (is SwitchAudioSource installed?)", file=sys.stderr)
            print("Install with: brew install switchaudio-osx", file=sys.stderr)

    try:
        # Create and start transcriber
        transcriber = LiveTranscriber(
            model_size=args.model,
            output_dir=args.output_dir,
            mic_device=mic_device,
            computer_device=computer_device,
            vad_aggressiveness=args.vad_aggressiveness,
            silence_duration=args.silence_duration,
            min_chunk_duration=args.min_chunk_duration,
            max_chunk_duration=args.max_chunk_duration
        )

        transcriber.start()
    finally:
        # Restore original audio output
        if original_output:
            print(f"\nRestoring output to: {original_output}", file=sys.stderr)
            if set_audio_output(original_output):
                print(f"✓ Restored to {original_output}", file=sys.stderr)
            else:
                print(f"Warning: Failed to restore to {original_output}", file=sys.stderr)

if __name__ == "__main__":
    main()
